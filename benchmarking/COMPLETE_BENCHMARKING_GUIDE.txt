================================================================================
COMPLETE YOLO BENCHMARKING GUIDE - WITH BASELINE COMPARISON
================================================================================

Updated: 2026-01-01
Status: ✅ Includes PyTorch Baseline (base-yolo) + All 3 NIMs

================================================================================
WHAT GETS BENCHMARKED (4 DEPLOYMENTS)
================================================================================

1. base-yolo       - PyTorch Baseline        (GPU 0) - Flask/Ultralytics
2. nim-binary      - TensorRT HTTP Binary    (GPU 1) - Triton Server
3. nim-grpc        - TensorRT Low Latency    (GPU 2) - Triton Server
4. nim-batching    - TensorRT High Throughput(GPU 3) - Triton Server

================================================================================
HOW IT WORKS
================================================================================

The benchmark suite automatically:

1. Detects deployment type and uses appropriate benchmark:
   - base-yolo:    Uses custom PyTorch benchmark (benchmark_base_yolo.py)
   - NIMs:         Uses universal Triton benchmark (benchmark_internal_universal.py)

2. Runs internal benchmarks (inside each pod):
   - base-yolo:    Calls /benchmark endpoint (built-in Flask route)
   - NIMs:         Uses tritonclient library (gRPC or HTTP)

3. Generates comprehensive report with:
   - Performance comparison table
   - TensorRT speedup calculations
   - Baseline vs optimized comparison
   - Recommendations for each use case

================================================================================
EXPECTED RESULTS
================================================================================

Based on A10 GPU performance:

Deployment      | Framework  | Mean Latency | FPS       | Speedup vs PyTorch
----------------|------------|--------------|-----------|--------------------
nim-binary      | TensorRT   | ~7-10 ms     | 100-140   | 8-12x faster
nim-grpc        | TensorRT   | ~8-12 ms     | 80-120    | 7-10x faster
nim-batching    | TensorRT   | ~8-12 ms     | 200-400*  | 7-10x faster
base-yolo       | PyTorch    | ~80-120 ms   | 8-12      | Baseline (1x)

* nim-batching throughput scales with concurrent requests

================================================================================
WHAT YOU'LL GET IN THE REPORT
================================================================================

1. PERFORMANCE SUMMARY TABLE
   ┌─────────────────────────────────────────────────────────────┐
   │ Deployment   | Framework | Mean | P95  | FPS   | Speedup   │
   ├─────────────────────────────────────────────────────────────┤
   │ nim-binary   | TensorRT  | 7.44 | 8.57 | 134.5 | 10.7x     │
   │ nim-grpc     | TensorRT  | 10.0 | 11.5 | 100.0 | 8.0x      │
   │ nim-batching | TensorRT  | 7.79 | 8.95 | 128.4 | 10.2x     │
   │ base-yolo    | PyTorch   | 80.0 | 95.0 | 12.5  | Baseline  │
   └─────────────────────────────────────────────────────────────┘

2. DETAILED STATISTICS
   - Min, Max, Mean, Median, P50, P90, P95, P99 latencies
   - Throughput (FPS)
   - Framework identification

3. ANALYSIS & RECOMMENDATIONS
   - Best deployment for low latency
   - Best deployment for high throughput
   - Best deployment for simplicity
   - TensorRT speedup vs PyTorch baseline
   - Deployment strategy comparison

4. KEY FINDINGS
   - "TensorRT NIMs are 8-10x faster than PyTorch"
   - "nim-binary achieves lowest latency (7.44ms)"
   - "nim-batching best for concurrent requests"

================================================================================
QUICK START (2 STEPS)
================================================================================

TERMINAL 1 - Port Forwarding:
────────────────────────────────────────────────────────────────
cd /path/to/benchmarking
./setup_port_forwarding.sh

# Keep running!
────────────────────────────────────────────────────────────────

TERMINAL 2 - Run Benchmark:
────────────────────────────────────────────────────────────────
cd /path/to/benchmarking
python3 benchmark_all_pods.py

# Wait 3-5 minutes for completion
────────────────────────────────────────────────────────────────

================================================================================
FILES IN BENCHMARKING SUITE
================================================================================

Core Scripts:
  benchmark_all_pods.py              - Main orchestrator
  benchmark_internal_universal.py    - Universal Triton benchmark (NIMs)
  benchmark_base_yolo.py             - Custom PyTorch benchmark (base-yolo)
  setup_port_forwarding.sh           - Port forwarding setup

Documentation:
  README.md                          - Full documentation
  QUICK_START.txt                    - Quick reference
  COMPLETE_BENCHMARKING_GUIDE.txt    - This file
  BASE_YOLO_NOTE.txt                 - base-yolo details

================================================================================
UNDERSTANDING THE SPEEDUP
================================================================================

Example calculation:
  base-yolo latency:  80 ms
  nim-binary latency: 7.44 ms
  Speedup: 80 / 7.44 = 10.7x

This means:
  ✓ TensorRT is 10.7x faster than PyTorch
  ✓ Process 10.7x more images in same time
  ✓ Or process same images using 10.7x less GPU time

Cost savings:
  - If PyTorch needs 10 GPUs → TensorRT needs ~1 GPU
  - If PyTorch costs $100/month → TensorRT costs ~$10/month

================================================================================
WHY BASE-YOLO IS SLOWER
================================================================================

base-yolo (PyTorch):
  ✗ No optimization (runs raw PyTorch model)
  ✗ No kernel fusion
  ✗ No INT8/FP16 quantization
  ✗ No CUDA graph optimization
  ✗ General-purpose inference

NIMs (TensorRT):
  ✓ Optimized kernels (fused operations)
  ✓ FP16 precision (2x faster, same accuracy)
  ✓ CUDA graphs (reduced kernel launch overhead)
  ✓ Layer fusion (fewer memory transfers)
  ✓ GPU-specific optimizations

================================================================================
WHEN TO USE EACH DEPLOYMENT
================================================================================

USE base-yolo WHEN:
  → Need baseline performance comparison
  → Testing new models (pre-optimization)
  → Development/debugging model behavior
  → Don't care about performance (testing only)

USE nim-binary WHEN:
  → Need simple HTTP-only deployment
  → Low to moderate request rate
  → Want lowest latency (7-10ms)
  → Don't want gRPC client complexity

USE nim-grpc WHEN:
  → Need absolute lowest latency (8-12ms)
  → Using gRPC clients
  → Real-time video processing
  → Interactive applications

USE nim-batching WHEN:
  → High concurrent request volume (100+ requests/sec)
  → Batch video processing
  → High-throughput production workloads
  → Can tolerate 5ms queue delay for better throughput

================================================================================
TROUBLESHOOTING
================================================================================

Issue: base-yolo benchmark fails
Solution: Check Flask server is running:
  kubectl logs -n yolo-base POD_NAME -c yolo-golden
  Should see: "Model loaded on GPU"

Issue: Port not 8080
Solution: base-yolo runs on port 8080, mapped to 80 externally
  Internal: 127.0.0.1:8080
  External: EXTERNAL_IP:80

Issue: Different latency numbers
Reason: PyTorch latency varies more than TensorRT
  - First run: slower (cold start)
  - After warmup: faster but still 8-10x slower than TensorRT

================================================================================
SAMPLE REPORT OUTPUT
================================================================================

================================================================================
YOLO NIM COMPREHENSIVE BENCHMARK REPORT
================================================================================
Generated: 2026-01-01 10:00:00
Iterations per test: 50

================================================================================
PERFORMANCE SUMMARY - All Deployments
================================================================================

Deployment      | Framework  | Mean (ms)  | P95 (ms)   | FPS      | Speedup
---------------------------------------------------------------------------------
nim-binary      | TENSORRT   |      7.44  |      8.57  |   134.5  | 10.7x
nim-batching    | TENSORRT   |      7.79  |      8.95  |   128.4  | 10.2x
nim-grpc        | TENSORRT   |     10.03  |     11.57  |    99.7  | 8.0x
base-yolo       | PYTORCH    |     80.25  |     95.40  |    12.5  | Baseline

================================================================================
ANALYSIS & RECOMMENDATIONS
================================================================================

Best NIM Performance:
  Lowest Latency:     nim-binary - 7.44 ms
  Highest Throughput: nim-binary - 134.5 FPS
  TensorRT Speedup:   10.7x faster than PyTorch baseline

Recommendations:

1. For LOWEST LATENCY (real-time applications):
   → Use nim-grpc with gRPC protocol
   → Expected: 8-12ms per request
   → Best for: Real-time video processing, interactive applications

2. For HIGHEST THROUGHPUT (batch processing):
   → Use nim-batching with gRPC protocol
   → Dynamic batching automatically groups requests
   → Expected: 200-400 FPS (depending on batch sizes)
   → Best for: High-load production, batch inference

3. For SIMPLICITY (HTTP-only):
   → Use nim-binary with HTTP protocol
   → No gRPC client library needed
   → Expected: 15-25ms per request
   → Best for: Simple integrations, testing

4. BASELINE COMPARISON (base-yolo):
   → PyTorch baseline latency: 80.25ms
   → TensorRT NIMs are 10.7x faster
   → Use for performance baseline comparison
   → Not recommended for production (slower than TensorRT)

================================================================================
KEY TAKEAWAY
================================================================================

✓ TensorRT NIMs deliver 8-10x speedup over PyTorch
✓ All NIMs achieve <12ms latency (vs 80ms PyTorch)
✓ Choose deployment based on workload:
  - nim-binary:   Best latency, simple HTTP
  - nim-grpc:     Best for gRPC clients
  - nim-batching: Best for high concurrent load

✓ base-yolo confirms massive TensorRT advantage!

================================================================================
